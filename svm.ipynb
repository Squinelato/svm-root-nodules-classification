{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from skimage.measure import moments_hu\n",
    "from mahotas.features import haralick\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "        self.X_train = None  \n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.cross_val = StratifiedKFold(n_splits=5)\n",
    "        self.optimized_classifier = None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_images_from_folder(folder):\n",
    "        images = list()\n",
    "        for filename in os.listdir(folder):\n",
    "            img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_features(image):\n",
    "        return np.r_[moments_hu(image), haralick(image).flatten()]\n",
    "\n",
    "    def save_extracted_features(self, bboxes_path):\n",
    "        positive_instance = self.load_images_from_folder(bboxes_path + 'nodules/')\n",
    "        negative_instance = self.load_images_from_folder(bboxes_path + 'non-nodules/')\n",
    "\n",
    "        positive_features = np.array(list(map(self.extract_features, positive_instance)))\n",
    "        negative_features = np.array(list(map(self.extract_features, negative_instance)))\n",
    "\n",
    "        features = np.r_[positive_features, negative_features]\n",
    "        labels = np.r_[np.ones(len(positive_instance)), np.zeros(len(negative_instance))]\n",
    "\n",
    "        np.save('features/features.npy', features)\n",
    "        np.save('features/labels.npy', labels)\n",
    "\n",
    "    def load_features(self):\n",
    "        self.features = np.load('features/features.npy')\n",
    "        self.labels = np.load('features/labels.npy')\n",
    "        \n",
    "    def split_dataset(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.features, \n",
    "                                                            self.labels, \n",
    "                                                            stratify=self.labels, \n",
    "                                                            test_size=0.2)\n",
    "        self.X_train = X_train  \n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def normalize(self):\n",
    "        scaler = RobustScaler()\n",
    "        self.X_train = scaler.fit_transform(self.X_train)\n",
    "        self.X_test = scaler.transform(self.X_test)\n",
    "        \n",
    "    def training(self, kernel, scoring='f1_weighted'):\n",
    "        \n",
    "        if kernel != 'linear':\n",
    "            default_classifier = SVC(class_weight='balanced', \n",
    "                                     decision_function_shape='ovo', cache_size=4000)\n",
    "            \n",
    "            default_params = {'C': np.reciprocal(np.arange(1, 10).astype(np.float)), \n",
    "                              'kernel': [kernel], 'gamma': ['scale'],\n",
    "                              'coef0': np.arange(0, 10, 0.1), 'degree': range(1, 10)}\n",
    "        else:\n",
    "            n = self.X_train.shape[0]\n",
    "            default_classifier = SGDClassifier(loss='hinge',\n",
    "                                               class_weight='balanced', \n",
    "                                               max_iter = np.ceil(10**6 / n),\n",
    "                                               shuffle = True)\n",
    "            \n",
    "            default_params = {'alpha'    : 10.0**-np.arange(1,7),\n",
    "                              'l1_ratio' : np.arange(0.00, 1.001, 0.001)}\n",
    "\n",
    "\n",
    "        grid_search = GridSearchCV(default_classifier,\n",
    "                                   param_grid=default_params,\n",
    "                                   cv=self.cross_val, scoring=scoring,\n",
    "                                   verbose=3, n_jobs=4)\n",
    "\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "        print('Best score: {}'.format(grid_search.best_score_))\n",
    "        print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "        now = dt.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "        joblib.dump(grid_search.best_estimator_, 'classifiers/{}_{}.plk'.format(kernel, now))\n",
    "        \n",
    "    def load_optimized_classifier(self, classifier_path):\n",
    "        self.optimized_classifier = joblib.load(classifier_path)\n",
    "        print('Parameters: {}'.format(self.optimized_classifier.get_params()))\n",
    "\n",
    "    def get_metrics(self, classifier):\n",
    "        scores = cross_validate(classifier, self.X_train, self.y_train, \n",
    "                                cv=self.cross_val, n_jobs=-1,\n",
    "                                scoring=['balanced_accuracy', 'f1_weighted'])\n",
    "        return (scores['test_balanced_accuracy'].mean(), \n",
    "                scores['test_balanced_accuracy'].std(), \n",
    "                scores['test_f1_weighted'].mean(), \n",
    "                scores['test_f1_weighted'].std())\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        classifiers = [(clf, joblib.load('./classifiers/' + clf)) for clf in os.listdir('./classifiers') if '.plk' in clf]\n",
    "        \n",
    "        headers = ['file_name', 'day', 'hour', 'balanced_accuracy', 'std', 'f1_weighted', 'std']\n",
    "\n",
    "        with open('scores/results.csv', 'w') as file:\n",
    "\n",
    "            file.writelines(','.join(headers) + '\\n')\n",
    "            for name, classifier in classifiers:\n",
    "                data = [str(metric) for metric in self.get_metrics(classifier)]\n",
    "                name = name.replace('.plk', '')\n",
    "                file.writelines(','.join(name.split('_')) + ',' + ','.join(data) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER = Identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIIER.save_extracted_features('bbox_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.load_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.split_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.calculate_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 8100 candidates, totalling 40500 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  56 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=4)]: Done 2808 tasks      | elapsed:    6.5s\n[Parallel(n_jobs=4)]: Done 7928 tasks      | elapsed:   18.0s\n[Parallel(n_jobs=4)]: Done 15096 tasks      | elapsed:   34.7s\n[Parallel(n_jobs=4)]: Done 24312 tasks      | elapsed:   56.2s\n[Parallel(n_jobs=4)]: Done 35576 tasks      | elapsed:  1.4min\nBest score: 0.782930402930403\nBest parameters: {'C': 0.5, 'coef0': 1.8, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'}\n[Parallel(n_jobs=4)]: Done 40500 out of 40500 | elapsed:  1.6min finished\n"
    }
   ],
   "source": [
    "IDENTIIER.training('poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.load_optimized_classifier('classifiers/linear.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondaec126aa0c77e42d49e9618db2a0d19bc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}