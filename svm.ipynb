{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"this module aims in load, processes and train SVM algoritm in root nodules dataset\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import cv2\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mahotas.features import haralick\n",
    "from skimage.measure import moments_hu\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold, train_test_split\n",
    "\n",
    "\n",
    "\n",
    "class Identifier:\n",
    "    \"\"\"this class is designed to load and preocess imagens and train SVM classifiers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.scaler = RobustScaler()\n",
    "        self.cross_val = StratifiedKFold(n_splits=5)\n",
    "        self.optimized_classifier = None\n",
    "        self.bbox_list = list()\n",
    "        self.y_predict = list()\n",
    "        self.image = None\n",
    "        self.csv_file = None\n",
    "        self.json_file = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_images_from_folder(folder):\n",
    "        \"\"\"this method can load all the imagens from a given folder\n",
    "\n",
    "        Arguments:\n",
    "            folder {str} -- the path of the folder\n",
    "\n",
    "        Returns:\n",
    "            list -- a list with the loaded images\n",
    "        \"\"\"\n",
    "        images = list()\n",
    "        for filename in os.listdir(folder):\n",
    "            img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        return images\n",
    "\n",
    "    def read_image(self, image_path):\n",
    "        \"\"\"this method read an input image turn it to a RGB representation and return its shape\n",
    "\n",
    "        Arguments:\n",
    "            image_path {Str} -- the input path of the given image\n",
    "\n",
    "        Returns:\n",
    "            list -- a list with the height, width and the number of color channels of the image\n",
    "        \"\"\"\n",
    "        original_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        self.image = original_image\n",
    "\n",
    "        return original_image.shape\n",
    "\n",
    "    def read_csv_file(self, csv_path):\n",
    "        \"\"\"this method read a csv file and get the position of each bounding box\n",
    "        of the image, besides its label, and save it on the class objects\n",
    "\n",
    "        Arguments:\n",
    "            csv_path {str} -- the input path of the csv file\n",
    "        \"\"\"\n",
    "        self.csv_file = pd.read_csv(csv_path)\n",
    "        for minr_norm, minc_norm, maxr_norm, maxc_norm in zip(self.csv_file.minr_norm,\n",
    "                                                              self.csv_file.minc_norm,\n",
    "                                                              self.csv_file.maxr_norm,\n",
    "                                                              self.csv_file.maxc_norm):\n",
    "\n",
    "            self.bbox_list.append([minr_norm, minc_norm, maxr_norm, maxc_norm])\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_features(image):\n",
    "        \"\"\"this method applies the hu moments and the haralick method on a given image\n",
    "\n",
    "        Arguments:\n",
    "            image {ndarray} -- the image whose the features will be extracted\n",
    "\n",
    "        Returns:\n",
    "            numpy.array -- an array with the extracted features\n",
    "        \"\"\"\n",
    "        return np.r_[moments_hu(image), haralick(image).flatten()]\n",
    "\n",
    "    def save_extracted_features(self, bboxes_path):\n",
    "        \"\"\"this method aggregate all the extracted features from all the images and saves\n",
    "        this data in the disk as a numpy file\n",
    "\n",
    "        Arguments:\n",
    "            bboxes_path {str} -- the path of the dataset folder\n",
    "        \"\"\"\n",
    "        positive_instance = self.load_images_from_folder(bboxes_path + 'nodules/')\n",
    "        negative_instance = self.load_images_from_folder(bboxes_path + 'non-nodules/')\n",
    "\n",
    "        positive_features = np.array(list(map(self.extract_features, positive_instance)))\n",
    "        negative_features = np.array(list(map(self.extract_features, negative_instance)))\n",
    "\n",
    "        features = np.r_[positive_features, negative_features]\n",
    "        labels = np.r_[np.ones(len(positive_instance)), np.zeros(len(negative_instance))]\n",
    "\n",
    "        np.save('features/features.npy', features)\n",
    "        np.save('features/labels.npy', labels)\n",
    "\n",
    "    def load_features(self):\n",
    "        \"\"\"this method loads the extracted features and its labels\n",
    "        \"\"\"\n",
    "        self.features = np.load('features/features.npy')\n",
    "        self.labels = np.load('features/labels.npy')\n",
    "\n",
    "    def split_dataset(self):\n",
    "        \"\"\"this method split the loaded features into a training and a test\n",
    "        dataset, in a stratified way\n",
    "        \"\"\"\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.features,\n",
    "                                                            self.labels,\n",
    "                                                            stratify=self.labels,\n",
    "                                                            test_size=0.2)\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def reset_attributes(self):\n",
    "        \"\"\"this method reset all the attributes of the Maker class\n",
    "        \"\"\"\n",
    "        self.image = None\n",
    "        self.y_predict = list()\n",
    "        self.bbox_list = list()\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"this method normalizes a given dataset\n",
    "        \"\"\"\n",
    "        self.scaler.fit(self.x_train)\n",
    "        self.x_train = self.scaler.transform(self.x_train)\n",
    "        self.x_test = self.scaler.transform(self.x_test)\n",
    "\n",
    "    def pipeline(self):\n",
    "        \"\"\"this method executes a pipeline with the load_features, split_dataset\n",
    "        and the normalize method\n",
    "        \"\"\"\n",
    "        print('Loading features')\n",
    "        self.load_features()\n",
    "        print('Splitting the dataset')\n",
    "        self.split_dataset()\n",
    "        print('Normalizing the dataset')\n",
    "        self.normalize()\n",
    "\n",
    "    def training(self, kernel, scoring='f1_weighted'):\n",
    "        \"\"\"this method train and fine-tune a SVM classfier with the given kernel\n",
    "\n",
    "        Arguments:\n",
    "            kernel {str} -- the desired kernel to train the SVM\n",
    "\n",
    "        Keyword Arguments:\n",
    "            scoring {str} -- the scoring method to evaluate the trainig (default: {'f1_weighted'})\n",
    "        \"\"\"\n",
    "        if kernel != 'linear':\n",
    "            default_classifier = SVC(class_weight='balanced',\n",
    "                                     decision_function_shape='ovo',\n",
    "                                     cache_size=4000)\n",
    "\n",
    "            default_params = {'C': np.reciprocal(np.arange(1, 10).astype(np.float)),\n",
    "                              'kernel': [kernel], 'gamma': ['scale'],\n",
    "                              'coef0': np.arange(0, 10, 0.1), 'degree': range(1, 10)}\n",
    "        else:\n",
    "            number_instances = self.x_train.shape[0]\n",
    "            default_classifier = SGDClassifier(loss='hinge',\n",
    "                                               class_weight='balanced',\n",
    "                                               max_iter=np.ceil(10**6 / number_instances),\n",
    "                                               shuffle=True)\n",
    "\n",
    "            default_params = {'alpha': 10.0**-np.arange(1, 7),\n",
    "                              'l1_ratio': np.arange(0.00, 1.001, 0.001)}\n",
    "\n",
    "        grid_search = GridSearchCV(default_classifier,\n",
    "                                   param_grid=default_params,\n",
    "                                   cv=self.cross_val, scoring=scoring,\n",
    "                                   verbose=3, n_jobs=4)\n",
    "\n",
    "        grid_search.fit(self.x_train, self.y_train)\n",
    "        print('Best score: {}'.format(grid_search.best_score_))\n",
    "        print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "        now = dt.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "        joblib.dump(grid_search.best_estimator_, 'classifiers/{}_{}.plk'.format(kernel, now))\n",
    "\n",
    "    def load_optimized_classifier(self, classifier_path):\n",
    "        \"\"\"this method can load a trained SVM from the disk\n",
    "\n",
    "        Arguments:\n",
    "            classifier_path {str} -- the path of the classifier\n",
    "        \"\"\"\n",
    "        self.optimized_classifier = joblib.load(classifier_path)\n",
    "        print('Parameters: {}'.format(self.optimized_classifier.get_params()))\n",
    "\n",
    "    def get_metrics(self, classifier):\n",
    "        \"\"\"this method calculate the balanced accuracy and the F1 score for\n",
    "        a given classifier\n",
    "\n",
    "        Arguments:\n",
    "            classifier {sklearn.classifier} -- the classifier to get the metrics\n",
    "\n",
    "        Returns:\n",
    "            list -- the mean and the standard deviation of the balanced accuracy and the F1 score\n",
    "        \"\"\"\n",
    "        scores = cross_validate(classifier, self.x_train, self.y_train,\n",
    "                                cv=self.cross_val, n_jobs=-1,\n",
    "                                scoring=['balanced_accuracy', 'f1_weighted'])\n",
    "\n",
    "        return (scores['test_balanced_accuracy'].mean(),\n",
    "                scores['test_balanced_accuracy'].std(),\n",
    "                scores['test_f1_weighted'].mean(),\n",
    "                scores['test_f1_weighted'].std())\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"this method uses the get_metrics method in order to evaluate all the classifiers\n",
    "        on a given folder and then compile these informations in a CSV file\n",
    "        \"\"\"\n",
    "        classifiers = [(clf, joblib.load('./classifiers/' + clf))\n",
    "                       for clf in os.listdir('./classifiers') if '.plk' in clf]\n",
    "\n",
    "        headers = ['file_name', 'day', 'hour', 'balanced_accuracy', 'std', 'f1_weighted', 'std']\n",
    "\n",
    "        with open('scores/results.csv', 'w') as file:\n",
    "\n",
    "            file.writelines(','.join(headers) + '\\n')\n",
    "            for name, classifier in classifiers:\n",
    "                data = [str(metric) for metric in self.get_metrics(classifier)]\n",
    "                name = name.replace('.plk', '')\n",
    "                file.writelines(','.join(name.split('_')) + ',' + ','.join(data) + '\\n')\n",
    "\n",
    "    def csv_predict(self, image_path, csv_path):\n",
    "        image_height, image_width = self.read_image(image_path)\n",
    "        self.read_csv_file(csv_path)\n",
    "\n",
    "        for bbox in self.bbox_list:\n",
    "\n",
    "            minr_norm, minc_norm, maxr_norm, maxc_norm = bbox\n",
    "\n",
    "            minr, maxr = int(minr_norm*image_height), int(maxr_norm*image_height)\n",
    "            minc, maxc = int(minc_norm*image_width), int(maxc_norm*image_width)\n",
    "\n",
    "            features = self.extract_features(self.image[minr:maxr, minc:maxc])\n",
    "            features = self.scaler.transform([features])\n",
    "\n",
    "            self.y_predict.append(int(self.optimized_classifier.predict(features)[0]))\n",
    "        \n",
    "        self.csv_file['y_label'] = self.y_predict\n",
    "        self.csv_file.to_csv(csv_path)\n",
    "\n",
    "    def json_predict(self, image_path, json_path):\n",
    "        image_height, image_width = self.read_image(image_path)\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.json_file = json.load(f)\n",
    "\n",
    "        for i, bbox in enumerate(self.json_file['bboxes']):\n",
    "\n",
    "            minr_norm, minc_norm, maxr_norm, maxc_norm = (bbox['rendering']['minr'],\n",
    "                                                          bbox['rendering']['minc'],\n",
    "                                                          bbox['rendering']['maxr'],\n",
    "                                                          bbox['rendering']['maxc'])\n",
    "\n",
    "            minr, maxr = int(minr_norm*image_height), int(maxr_norm*image_height)\n",
    "            minc, maxc = int(minc_norm*image_width), int(maxc_norm*image_width)\n",
    "\n",
    "            features = self.extract_features(self.image[minr:maxr, minc:maxc])\n",
    "            features = self.scaler.transform([features])\n",
    "\n",
    "            if not int(self.optimized_classifier.predict(features)[0]):\n",
    "                del self.json_file['bboxes'][i]\n",
    "\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(self.json_file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIFIER = Identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading features\nSplitting the dataset\nNormalizing the dataset\n"
    }
   ],
   "source": [
    "IDENTIFIER.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Parameters: {'C': 0.5, 'break_ties': False, 'cache_size': 4000, 'class_weight': 'balanced', 'coef0': 1.8, 'decision_function_shape': 'ovo', 'degree': 2, 'gamma': 'scale', 'kernel': 'poly', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n"
    }
   ],
   "source": [
    "IDENTIFIER.load_optimized_classifier('classifiers/poly_2020-04-21_21:18:09.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n0\n0\n"
    }
   ],
   "source": [
    "IDENTIFIER.json_predict('images/model_2.png', 'images/model_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('images/model_2.json', 'r') as f:\n",
    "    my_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "13"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "len(my_json['bboxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(my_json['bboxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'bboxID': 6165141332427712469,\n 'position': {'center_x': 0.6072970639032815,\n  'center_y': 0.3249616564417178,\n  'width': 0.02310017271157161,\n  'height': 0.042177914110429426},\n 'rendering': {'minc': 0.5957469775474957,\n  'minr': 0.3038726993865031,\n  'maxc': 0.6188471502590673,\n  'maxr': 0.3460506134969325}}"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "my_json['bboxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del my_json['bboxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'bboxID': 6860819852570822625,\n 'position': {'center_x': 0.6599740932642487,\n  'center_y': 0.3546779141104294,\n  'width': 0.02223661485319517,\n  'height': 0.03412576687116564},\n 'rendering': {'minc': 0.6488557858376511,\n  'minr': 0.3376150306748466,\n  'maxc': 0.6710924006908463,\n  'maxr': 0.37174079754601225}}"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "my_json['bboxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIIER.save_extracted_features('bbox_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.load_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.split_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.calculate_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 8100 candidates, totalling 40500 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  56 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=4)]: Done 2808 tasks      | elapsed:    6.5s\n[Parallel(n_jobs=4)]: Done 7928 tasks      | elapsed:   18.0s\n[Parallel(n_jobs=4)]: Done 15096 tasks      | elapsed:   34.7s\n[Parallel(n_jobs=4)]: Done 24312 tasks      | elapsed:   56.2s\n[Parallel(n_jobs=4)]: Done 35576 tasks      | elapsed:  1.4min\nBest score: 0.782930402930403\nBest parameters: {'C': 0.5, 'coef0': 1.8, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'}\n[Parallel(n_jobs=4)]: Done 40500 out of 40500 | elapsed:  1.6min finished\n"
    }
   ],
   "source": [
    "IDENTIIER.training('poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIIER.load_optimized_classifier('classifiers/linear.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondaec126aa0c77e42d49e9618db2a0d19bc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}